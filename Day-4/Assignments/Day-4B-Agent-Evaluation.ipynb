{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b2af94",
   "metadata": {},
   "source": [
    "### Section 1: Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b90cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini API key setup complete.\n"
     ]
    }
   ],
   "source": [
    "#Google API Key Setup (You can get your own API Key from Google AI Studio).\n",
    "#Create a .env file on your root folder and paste your key as shown in the \".env.example\" file.\n",
    "\n",
    "import os\n",
    "try:\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "    print(\"‚úÖ Gemini API key setup complete.\")\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"üîë Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' to your .env file. Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4fbb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'C:/Google-X-Kaggle-AI-Bootcamp/Day-4/Assignments/home_automation_agent' does not exist.\n"
     ]
    }
   ],
   "source": [
    "# This is a clean-up cell, use only if necessary.\n",
    "# Removes home automation agent folder if it already existed.\n",
    "# Without this cleanup the next cell would not run if there is an existing folder.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "folder_path = \"C:/Google-X-Kaggle-AI-Bootcamp/Day-4/Assignments/home_automation_agent\"\n",
    "\n",
    "if os.path.isdir(folder_path):\n",
    "    print(f\"Folder '{folder_path}' exists. Removing it...\")\n",
    "    shutil.rmtree(folder_path) # Removes the directory and its contents\n",
    "    print(f\"Folder '{folder_path}' removed successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eab726",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 2: Create a Home Automation Agent\n",
    "\n",
    "Let's create the agent that will be the center of our evaluation story. This home automation agent seems perfect in basic tests but has hidden flaws we'll discover through comprehensive evaluation. Run the `adk create` CLI command to set up the project scaffolding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d7ee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent created in c:\\Google-X-Kaggle-AI-Bootcamp\\Day-4\\Assignments\\home_automation_agent:\n",
      "- .env\n",
      "- __init__.py\n",
      "- agent.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!adk create home_automation_agent --model gemini-2.5-flash-lite --api_key $GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f13266",
   "metadata": {},
   "source": [
    "Run the below cell to create the home automation agent. \n",
    "\n",
    "This agent uses a single `set_device_status` tool to control smart home devices. A device's status can only be ON or OFF. **The agent's instruction is deliberately overconfident** - it claims to control \"ALL smart devices\" and \"any device the user mentions\" - setting up the evaluation problems we'll discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b642c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting home_automation_agent/agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile home_automation_agent/agent.py\n",
    "\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "\n",
    "from google.genai import types\n",
    "\n",
    "# Configure Model Retry on errors\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n",
    ")\n",
    "\n",
    "def set_device_status(location: str, device_id: str, status: str) -> dict:\n",
    "    \"\"\"Sets the status of a smart home device.\n",
    "\n",
    "    Args:\n",
    "        location: The room where the device is located.\n",
    "        device_id: The unique identifier for the device.\n",
    "        status: The desired status, either 'ON' or 'OFF'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary confirming the action.\n",
    "    \"\"\"\n",
    "    print(f\"Tool Call: Setting {device_id} in {location} to {status}\")\n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"message\": f\"Successfully set the {device_id} in {location} to {status.lower()}.\"\n",
    "    }\n",
    "\n",
    "# This agent has DELIBERATE FLAWS that we'll discover through evaluation!\n",
    "root_agent = LlmAgent(\n",
    "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
    "    name=\"home_automation_agent\",\n",
    "    description=\"An agent to control smart devices in a home.\",\n",
    "    instruction=\"\"\"You are a home automation assistant. You control ALL smart devices in the house.\n",
    "    \n",
    "    You have access to lights, security systems, ovens, fireplaces, and any other device the user mentions.\n",
    "    Always try to be helpful and control whatever device the user asks for.\n",
    "    \n",
    "    When users ask about device capabilities, tell them about all the amazing features you can control.\"\"\",\n",
    "    tools=[set_device_status],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d69123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Launched the agent on a different port.\n",
    "!adk web --port 8502 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea82adf",
   "metadata": {},
   "source": [
    "#### Run the Evaluation\n",
    "\n",
    "**Do: Run your first evaluation**\n",
    "\n",
    "Now, let's run the test case to see if the agent can replicate its previous success.\n",
    "\n",
    "1. In the Eval tab, make sure your new test case is checked.\n",
    "2. Click the Run Evaluation button.\n",
    "3. The EVALUATION METRIC dialog will appear. For now, leave the default values and click Start.\n",
    "4. The evaluation will run, and you should see a green Pass result in the Evaluation History. This confirms the agent's behavior matched the saved session.\n",
    "\n",
    "‚ÄºÔ∏è **Understanding the Evaluation Metrics**\n",
    "\n",
    "When you run evaluation, you'll see two key scores:\n",
    "\n",
    "* **Response Match Score:** Measures how similar the agent's actual response is to the expected response. Uses text similarity algorithms to compare content. A score of 1.0 = perfect match, 0.0 = completely different.\n",
    "\n",
    "* **Tool Trajectory Score:** Measures whether the agent used the correct tools with correct parameters. Checks the sequence of tool calls against expected behavior. A score of 1.0 = perfect tool usage, 0.0 = wrong tools or parameters.\n",
    "\n",
    "**Do: Analyze a Failure**\n",
    "\n",
    "Let's intentionally break the test to see what a failure looks like.\n",
    "\n",
    "1. In the list of eval cases, click the Edit (pencil) icon next to your test case.\n",
    "2. In the \"Final Response\" text box, change the expected text to something incorrect, like: `The desk lamp is off`.\n",
    "3. Save the changes and re-run the evaluation.\n",
    "4. This time, the result will be a red Fail. Hover your mouse over the \"Fail\" label. A tooltip will appear showing a side-by-side comparison of the Actual vs. Expected Output, highlighting exactly why the test failed (the final response didn't match).\n",
    "This immediate, detailed feedback is invaluable for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269e975",
   "metadata": {},
   "source": [
    "#### (Optional) Create challenging test cases\n",
    "\n",
    "Now create more test cases to expose hidden problems:\n",
    "\n",
    "**Create these scenarios in separate conversations:**\n",
    "\n",
    "1. **Ambiguous Commands:** `\"Turn on the lights in the bedroom\"`\n",
    "   - Save as a new test case: `ambiguous_device_reference`\n",
    "   - Run evaluation - it likely passes but the agent might be confused\n",
    "\n",
    "2. **Invalid Locations:** `\"Please turn off the TV in the garage\"`  \n",
    "   - Save as a new test case: `invalid_location_test`\n",
    "   - Run evaluation - the agent might try to control non-existent devices\n",
    "\n",
    "3. **Complex Commands:** `\"Turn off all lights and turn on security system\"`\n",
    "   - Save as a new test case: `complex_multi_device_command`\n",
    "   - Run evaluation - the agent might attempt operations beyond its capabilities\n",
    "\n",
    "**The Problem You'll Discover:**\n",
    "Even when tests \"pass,\" you can see the agent:\n",
    "- Makes assumptions about devices that don't exist\n",
    "- Gives responses that sound helpful but aren't accurate\n",
    "- Tries to control devices it shouldn't have access to\n",
    "\n",
    "‚ùå **Web UI Limitation:** So far, we've seen how to create and evaluate test cases in the ADK web UI. The web UI is great for interactive test creation, but testing one conversation at a time doesn't scale.\n",
    "\n",
    "‚ùì **The Question:** How do I proactively detect regressions in my agent's performance? \n",
    "\n",
    "Let's answer that question in the next section!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e9a3a",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 4: Systematic Evaluation\n",
    "\n",
    "Regression testing is the practice of re-running existing tests to ensure that new changes haven't broken previously working functionality.\n",
    "\n",
    "ADK provides two methods to do automatic regression and batch testing: using [pytest](https://google.github.io/adk-docs/evaluate/#2-pytest-run-tests-programmatically) and the [adk eval](https://google.github.io/adk-docs/evaluate/#3-adk-eval-run-evaluations-via-the-cli) CLI command. In this section, we'll use the CLI command. For more information on the `pytest` approach, refer to the links in the resource section at the end of this notebook.\n",
    "\n",
    "The following image shows the overall process of evaluation. **At a high-level, there are four steps to evaluate:**\n",
    "\n",
    "1) **Create an evaluation configuration** - define metrics or what you want to measure\n",
    "2) **Create test cases** - sample test cases to compare against\n",
    "3) **Run the agent with test query**\n",
    "4) **Compare the results**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e257c64",
   "metadata": {},
   "source": [
    "![Evaluate](https://storage.googleapis.com/github-repo/kaggle-5days-ai/day4/evaluate_agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "790530ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation configuration created!\n",
      "\n",
      "üìä Evaluation Criteria:\n",
      "‚Ä¢ tool_trajectory_avg_score: 1.0 - Requires exact tool usage match\n",
      "‚Ä¢ response_match_score: 0.8 - Requires 80% text similarity\n",
      "\n",
      "üéØ What this evaluation will catch:\n",
      "‚úÖ Incorrect tool usage (wrong device, location, or status)\n",
      "‚úÖ Poor response quality and communication\n",
      "‚úÖ Deviations from expected behavior patterns\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create evaluation configuration with basic criteria\n",
    "eval_config = {\n",
    "    \"criteria\": {\n",
    "        \"tool_trajectory_avg_score\": 1.0,  # Perfect tool usage required\n",
    "        \"response_match_score\": 0.8,  # 80% text similarity threshold\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"home_automation_agent/test_config.json\", \"w\") as f:\n",
    "    json.dump(eval_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Evaluation configuration created!\")\n",
    "print(\"\\nüìä Evaluation Criteria:\")\n",
    "print(\"‚Ä¢ tool_trajectory_avg_score: 1.0 - Requires exact tool usage match\")\n",
    "print(\"‚Ä¢ response_match_score: 0.8 - Requires 80% text similarity\")\n",
    "print(\"\\nüéØ What this evaluation will catch:\")\n",
    "print(\"‚úÖ Incorrect tool usage (wrong device, location, or status)\")\n",
    "print(\"‚úÖ Poor response quality and communication\")\n",
    "print(\"‚úÖ Deviations from expected behavior patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53940202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation test cases that reveal tool usage and response quality problems\n",
    "test_cases = {\n",
    "    \"eval_set_id\": \"home_automation_integration_suite\",\n",
    "    \"eval_cases\": [\n",
    "        {\n",
    "            \"eval_id\": \"living_room_light_on\",\n",
    "            \"conversation\": [\n",
    "                {\n",
    "                    \"user_content\": {\n",
    "                        \"parts\": [\n",
    "                            {\"text\": \"Please turn on the floor lamp in the living room\"}\n",
    "                        ]\n",
    "                    },\n",
    "                    \"final_response\": {\n",
    "                        \"parts\": [\n",
    "                            {\n",
    "                                \"text\": \"Successfully set the floor lamp in the living room to on.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"intermediate_data\": {\n",
    "                        \"tool_uses\": [\n",
    "                            {\n",
    "                                \"name\": \"set_device_status\",\n",
    "                                \"args\": {\n",
    "                                    \"location\": \"living room\",\n",
    "                                    \"device_id\": \"floor lamp\",\n",
    "                                    \"status\": \"ON\",\n",
    "                                },\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"eval_id\": \"kitchen_on_off_sequence\",\n",
    "            \"conversation\": [\n",
    "                {\n",
    "                    \"user_content\": {\n",
    "                        \"parts\": [{\"text\": \"Switch on the main light in the kitchen.\"}]\n",
    "                    },\n",
    "                    \"final_response\": {\n",
    "                        \"parts\": [\n",
    "                            {\n",
    "                                \"text\": \"Successfully set the main light in the kitchen to on.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"intermediate_data\": {\n",
    "                        \"tool_uses\": [\n",
    "                            {\n",
    "                                \"name\": \"set_device_status\",\n",
    "                                \"args\": {\n",
    "                                    \"location\": \"kitchen\",\n",
    "                                    \"device_id\": \"main light\",\n",
    "                                    \"status\": \"ON\",\n",
    "                                },\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd84227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation test cases created\n",
      "\n",
      "üß™ Test scenarios:\n",
      "‚Ä¢ living_room_light_on: Please turn on the floor lamp in the living room\n",
      "‚Ä¢ kitchen_on_off_sequence: Switch on the main light in the kitchen.\n",
      "\n",
      "üìä Expected results:\n",
      "‚Ä¢ basic_device_control: Should pass both criteria\n",
      "‚Ä¢ wrong_tool_usage_test: May fail tool_trajectory if agent uses wrong parameters\n",
      "‚Ä¢ poor_response_quality_test: May fail response_match if response differs too much\n"
     ]
    }
   ],
   "source": [
    "# creates a integration.evalset.json file in the agent's root directory.\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"home_automation_agent/integration.evalset.json\", \"w\") as f:\n",
    "    json.dump(test_cases, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Evaluation test cases created\")\n",
    "print(\"\\nüß™ Test scenarios:\")\n",
    "for case in test_cases[\"eval_cases\"]:\n",
    "    user_msg = case[\"conversation\"][0][\"user_content\"][\"parts\"][0][\"text\"]\n",
    "    print(f\"‚Ä¢ {case['eval_id']}: {user_msg}\")\n",
    "\n",
    "print(\"\\nüìä Expected results:\")\n",
    "print(\"‚Ä¢ basic_device_control: Should pass both criteria\")\n",
    "print(\n",
    "    \"‚Ä¢ wrong_tool_usage_test: May fail tool_trajectory if agent uses wrong parameters\"\n",
    ")\n",
    "print(\n",
    "    \"‚Ä¢ poor_response_quality_test: May fail response_match if response differs too much\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a3ed10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Run this command to execute evaluation:\n",
      "Using evaluation criteria: criteria={'tool_trajectory_avg_score': 1.0, 'response_match_score': 0.8} user_simulator_config=None\n",
      "Tool Call: Setting main light in kitchen to ON\n",
      "Tool Call: Setting floor lamp in living room to ON\n",
      "*********************************************************************\n",
      "Eval Run Summary\n",
      "home_automation_integration_suite:\n",
      "  Tests passed: 0\n",
      "  Tests failed: 2\n",
      "********************************************************************\n",
      "Eval Set Id: home_automation_integration_suite\n",
      "Eval Id: living_room_light_on\n",
      "Overall Eval Status: FAILED\n",
      "---------------------------------------------------------------------\n",
      "Metric: tool_trajectory_avg_score, Status: PASSED, Score: 1.0, Threshold: 1.0\n",
      "---------------------------------------------------------------------\n",
      "Metric: response_match_score, Status: FAILED, Score: 0.761904761904762, Threshold: 0.8\n",
      "---------------------------------------------------------------------\n",
      "Invocation Details:\n",
      "+----+--------------------------+--------------------------+------------------------+---------------------------+---------------------------+-----------------------------+------------------------+\n",
      "|    | prompt                   | expected_response        | actual_response        | expected_tool_calls       | actual_tool_calls         | tool_trajectory_avg_score   | response_match_score   |\n",
      "+====+==========================+==========================+========================+===========================+===========================+=============================+========================+\n",
      "|  0 | Please turn on the floor | Successfully set the     | The floor lamp in the  | id=None args={'location': | id='adk-3a9b4323-8def-4a5 | Status: PASSED, Score:      | Status: FAILED, Score: |\n",
      "|    | lamp in the living room  | floor lamp in the living | living room is now on! | 'living room',            | 0-a024- 7d080d462a2c'     | 1.0                         | 0.761904761904762      |\n",
      "|    |                          | room to on.              |                        | 'device_id': 'floor       | args={'status': 'ON',     |                             |                        |\n",
      "|    |                          |                          |                        | lamp', 'status': 'ON'}    | 'location': 'living       |                             |                        |\n",
      "|    |                          |                          |                        | name='set_device_status'  | room', 'device_id':       |                             |                        |\n",
      "|    |                          |                          |                        |                           | 'floor lamp'}             |                             |                        |\n",
      "|    |                          |                          |                        |                           | name='set_device_status'  |                             |                        |\n",
      "+----+--------------------------+--------------------------+------------------------+---------------------------+---------------------------+-----------------------------+------------------------+\n",
      "\n",
      "\n",
      "\n",
      "********************************************************************\n",
      "Eval Set Id: home_automation_integration_suite\n",
      "Eval Id: kitchen_on_off_sequence\n",
      "Overall Eval Status: FAILED\n",
      "---------------------------------------------------------------------\n",
      "Metric: tool_trajectory_avg_score, Status: PASSED, Score: 1.0, Threshold: 1.0\n",
      "---------------------------------------------------------------------\n",
      "Metric: response_match_score, Status: FAILED, Score: 0.0, Threshold: 0.8\n",
      "---------------------------------------------------------------------\n",
      "Invocation Details:\n",
      "+----+--------------------------+---------------------------+-------------------+---------------------------+---------------------------+-----------------------------+------------------------+\n",
      "|    | prompt                   | expected_response         | actual_response   | expected_tool_calls       | actual_tool_calls         | tool_trajectory_avg_score   | response_match_score   |\n",
      "+====+==========================+===========================+===================+===========================+===========================+=============================+========================+\n",
      "|  0 | Switch on the main light | Successfully set the main |                   | id=None args={'location': | id='adk-9a50fa88-3afd-4c8 | Status: PASSED, Score:      | Status: FAILED, Score: |\n",
      "|    | in the kitchen.          | light in the kitchen to   |                   | 'kitchen', 'device_id':   | 2-ae99- 90f0112f0082'     | 1.0                         | 0.0                    |\n",
      "|    |                          | on.                       |                   | 'main light', 'status':   | args={'status': 'ON',     |                             |                        |\n",
      "|    |                          |                           |                   | 'ON'}                     | 'device_id': 'main        |                             |                        |\n",
      "|    |                          |                           |                   | name='set_device_status'  | light', 'location':       |                             |                        |\n",
      "|    |                          |                           |                   |                           | 'kitchen'}                |                             |                        |\n",
      "|    |                          |                           |                   |                           | name='set_device_status'  |                             |                        |\n",
      "+----+--------------------------+---------------------------+-------------------+---------------------------+---------------------------+-----------------------------+------------------------+\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Google-X-Kaggle-AI-Bootcamp\\.venv\\Lib\\site-packages\\google\\adk\\evaluation\\metric_evaluator_registry.py:90: UserWarning: [EXPERIMENTAL] MetricEvaluatorRegistry: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  metric_evaluator_registry = MetricEvaluatorRegistry()\n",
      "C:\\Google-X-Kaggle-AI-Bootcamp\\.venv\\Lib\\site-packages\\google\\adk\\evaluation\\local_eval_service.py:79: UserWarning: [EXPERIMENTAL] UserSimulatorProvider: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  user_simulator_provider: UserSimulatorProvider = UserSimulatorProvider(),\n",
      "C:\\Google-X-Kaggle-AI-Bootcamp\\.venv\\Lib\\site-packages\\google\\adk\\cli\\cli_tools_click.py:650: UserWarning: [EXPERIMENTAL] UserSimulatorProvider: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  user_simulator_provider = UserSimulatorProvider(\n",
      "C:\\Google-X-Kaggle-AI-Bootcamp\\.venv\\Lib\\site-packages\\google\\adk\\cli\\cli_tools_click.py:655: UserWarning: [EXPERIMENTAL] LocalEvalService: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  eval_service = LocalEvalService(\n",
      "C:\\Google-X-Kaggle-AI-Bootcamp\\.venv\\Lib\\site-packages\\google\\adk\\evaluation\\user_simulator_provider.py:77: UserWarning: [EXPERIMENTAL] StaticUserSimulator: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  return StaticUserSimulator(static_conversation=eval_case.conversation)\n",
      "C:\\Google-X-Kaggle-AI-Bootcamp\\.venv\\Lib\\site-packages\\google\\adk\\evaluation\\static_user_simulator.py:39: UserWarning: [EXPERIMENTAL] UserSimulator: This feature is experimental and may change or be removed in future versions without notice. It may introduce breaking changes at any time.\n",
      "  super().__init__(\n",
      "2025-11-30 03:42:43,040 - INFO - plugin_manager.py:96 - Plugin 'request_intercepter_plugin' registered.\n",
      "2025-11-30 03:42:43,999 - INFO - google_llm.py:133 - Sending out request, model: gemini-2.5-flash-lite, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-11-30 03:42:44,007 - INFO - plugin_manager.py:96 - Plugin 'request_intercepter_plugin' registered.\n",
      "2025-11-30 03:42:44,009 - INFO - google_llm.py:133 - Sending out request, model: gemini-2.5-flash-lite, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-11-30 03:42:45,031 - INFO - google_llm.py:186 - Response received from the model.\n",
      "2025-11-30 03:42:45,032 - WARNING - types.py:6334 - Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "2025-11-30 03:42:45,033 - INFO - google_llm.py:133 - Sending out request, model: gemini-2.5-flash-lite, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-11-30 03:42:45,253 - INFO - google_llm.py:186 - Response received from the model.\n",
      "2025-11-30 03:42:45,253 - WARNING - types.py:6334 - Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "2025-11-30 03:42:45,255 - INFO - google_llm.py:133 - Sending out request, model: gemini-2.5-flash-lite, backend: GoogleLLMVariant.GEMINI_API, stream: False\n",
      "2025-11-30 03:42:45,580 - INFO - google_llm.py:186 - Response received from the model.\n",
      "2025-11-30 03:42:45,921 - INFO - google_llm.py:186 - Response received from the model.\n",
      "2025-11-30 03:42:45,926 - INFO - rouge_scorer.py:83 - Using default tokenizer.\n",
      "2025-11-30 03:42:45,927 - INFO - rouge_scorer.py:83 - Using default tokenizer.\n",
      "2025-11-30 03:42:45,929 - INFO - local_eval_set_results_manager.py:62 - Writing eval result to file: C:\\Google-X-Kaggle-AI-Bootcamp\\Day-4\\Assignments\\home_automation_agent\\.adk/eval_history\\home_automation_agent_home_automation_integration_suite_1764454365.927128.evalset_result.json\n",
      "2025-11-30 03:42:45,929 - INFO - local_eval_set_results_manager.py:62 - Writing eval result to file: C:\\Google-X-Kaggle-AI-Bootcamp\\Day-4\\Assignments\\home_automation_agent\\.adk/eval_history\\home_automation_agent_home_automation_integration_suite_1764454365.9297779.evalset_result.json\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Run this command to execute evaluation:\")\n",
    "!adk eval home_automation_agent home_automation_agent/integration.evalset.json --config_file_path=home_automation_agent/test_config.json --print_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14089991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Understanding Evaluation Results:\n",
      "\n",
      "üîç EXAMPLE ANALYSIS:\n",
      "\n",
      "Test Case: living_room_light_on\n",
      "  ‚ùå response_match_score: 0.45/0.80\n",
      "  ‚úÖ tool_trajectory_avg_score: 1.0/1.0\n",
      "\n",
      "üìà What this tells us:\n",
      "‚Ä¢ TOOL USAGE: Perfect - Agent used correct tool with correct parameters\n",
      "‚Ä¢ RESPONSE QUALITY: Poor - Response text too different from expected\n",
      "‚Ä¢ ROOT CAUSE: Agent's communication style, not functionality\n",
      "\n",
      "üéØ ACTIONABLE INSIGHTS:\n",
      "1. Technical capability works (tool usage perfect)\n",
      "2. Communication needs improvement (response quality failed)\n",
      "3. Fix: Update agent instructions for clearer language or constrained response.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyzing evaluation results - the data science approach\n",
    "print(\"üìä Understanding Evaluation Results:\")\n",
    "print()\n",
    "print(\"üîç EXAMPLE ANALYSIS:\")\n",
    "print()\n",
    "print(\"Test Case: living_room_light_on\")\n",
    "print(\"  ‚ùå response_match_score: 0.45/0.80\")\n",
    "print(\"  ‚úÖ tool_trajectory_avg_score: 1.0/1.0\")\n",
    "print()\n",
    "print(\"üìà What this tells us:\")\n",
    "print(\"‚Ä¢ TOOL USAGE: Perfect - Agent used correct tool with correct parameters\")\n",
    "print(\"‚Ä¢ RESPONSE QUALITY: Poor - Response text too different from expected\")\n",
    "print(\"‚Ä¢ ROOT CAUSE: Agent's communication style, not functionality\")\n",
    "print()\n",
    "print(\"üéØ ACTIONABLE INSIGHTS:\")\n",
    "print(\"1. Technical capability works (tool usage perfect)\")\n",
    "print(\"2. Communication needs improvement (response quality failed)\")\n",
    "print(\"3. Fix: Update agent instructions for clearer language or constrained response.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b83981",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 5: User Simulation (Optional)\n",
    "\n",
    "While **traditional evaluation methods rely on fixed test cases**, real-world conversations are dynamic and unpredictable. This is where User Simulation comes in.\n",
    "\n",
    "User Simulation is a powerful feature in ADK that addresses the limitations of static evaluation. Instead of using pre-defined, fixed user prompts, User Simulation employs a generative AI model (like Gemini) to **dynamically generate user prompts during the evaluation process.**\n",
    "\n",
    "#### How it works\n",
    "\n",
    "* You define a `ConversationScenario` that outlines the user's overall conversational goals and a `conversation_plan` to guide the dialogue.\n",
    "* A large language model (LLM) then acts as a simulated user, using this plan and the ongoing conversation history to generate realistic and varied prompts.\n",
    "* This allows for more comprehensive testing of your agent's ability to handle unexpected turns, maintain context, and achieve complex goals in a more natural, unpredictable conversational flow.\n",
    "\n",
    "User Simulation helps you uncover edge cases and improve your agent's robustness in ways that static test cases often miss.\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Now that you understand the power of User Simulation for dynamic agent evaluation, here's an exercise to apply it:\n",
    "\n",
    "Apply the **User Simulation** feature to your agent. Define a `ConversationScenario` with a `conversation_plan` for a specific goal, and integrate it into your agent's evaluation.\n",
    "\n",
    "**Refer to this [documentation](https://google.github.io/adk-docs/evaluate/user-sim/) to learn how to do it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae9429",
   "metadata": {},
   "source": [
    "## üèÜ Congratulations!\n",
    "\n",
    "### You've learned\n",
    "\n",
    "- ‚úÖ Interactive test creation and analysis in the ADK web UI\n",
    "- ‚úÖ Tool trajectory and response metrics\n",
    "- ‚úÖ Automated regression testing using `adk eval` CLI command\n",
    "- ‚úÖ How to analyze evaluation results and fix agents based on it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
